{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from dataset import SkipgramDataset\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Helper functions below. DO NOT MODIFY!    #\n",
    "#############################################\n",
    "\n",
    "\n",
    "class Word2Vec(torch.nn.Module, ABC):\n",
    "    \"\"\"\n",
    "    A helper class that wraps your word2vec losses.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_tokens: int, word_dimension: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.center_vectors = torch.nn.Parameter(torch.empty([n_tokens, word_dimension]))\n",
    "        self.outside_vectors = torch.nn.Parameter(torch.empty([n_tokens, word_dimension]))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        torch.nn.init.normal_(self.center_vectors.data)\n",
    "        torch.nn.init.normal_(self.outside_vectors.data)\n",
    "\n",
    "        \n",
    "class NegSamplingWord2Vec(Word2Vec):\n",
    "    def __init__(self, n_tokens: int, word_dimension: int, negative_sampler, K: int=10):\n",
    "        super().__init__(n_tokens, word_dimension)\n",
    "\n",
    "        self._negative_sampler = negative_sampler\n",
    "        self._K = K\n",
    "\n",
    "    def forward(self, center_word_index: torch.Tensor, outside_word_indices: torch.Tensor):\n",
    "        return neg_sampling_loss(self.center_vectors, self.outside_vectors, center_word_index, outside_word_indices, self._negative_sampler, self._K)\n",
    "\n",
    "#############################################\n",
    "# Testing functions below.                  #\n",
    "#############################################\n",
    "\n",
    "\n",
    "def test_neg_sampling_loss():\n",
    "    print (\"======Negative Sampling Loss Test Case======\")\n",
    "    center_word_index = torch.randint(1, 100, [5])\n",
    "    outside_word_indices = []\n",
    "    for _ in range(5):\n",
    "        random_window_size = random.randint(3, 6)\n",
    "        outside_word_indices.append([random.randint(1, 99) for _ in range(random_window_size)] + [0] * (6 - random_window_size))\n",
    "    outside_word_indices = torch.Tensor(outside_word_indices).to(torch.long)\n",
    "\n",
    "    neg_sampling_prob = torch.ones([100])\n",
    "    neg_sampling_prob[0] = 0.\n",
    "\n",
    "    dummy_database = type('dummy', (), {'_neg_sample_prob': neg_sampling_prob})\n",
    "\n",
    "    sampled_negatives = list()\n",
    "\n",
    "    def negative_sampler_wrapper(outside_word_indices, K):\n",
    "        result = SkipgramDataset.negative_sampler(dummy_database, outside_word_indices, K)\n",
    "        sampled_negatives.clear()\n",
    "        sampled_negatives.append(result)\n",
    "        return result\n",
    "\n",
    "    model = NegSamplingWord2Vec(n_tokens=100, word_dimension=3, negative_sampler=negative_sampler_wrapper, K=5)\n",
    "\n",
    "    loss = model(center_word_index, outside_word_indices).mean()\n",
    "    loss.backward()\n",
    "\n",
    "    # first test\n",
    "    assert (model.center_vectors.grad[0, :] == 0).all() and (model.outside_vectors.grad[0, :] == 0).all(), \\\n",
    "        \"<PAD> token should not affect the result.\"\n",
    "    print(\"The first test passed! Howerver, this test dosen't guarantee you that <PAD> tokens really don't affects result.\")    \n",
    "\n",
    "    # Second test\n",
    "    temp = model.center_vectors.grad.clone().detach()\n",
    "    temp[center_word_index] = 0.\n",
    "    assert (temp == 0.).all() and (model.center_vectors.grad[center_word_index] != 0.).all(), \\\n",
    "        \"Only batched center words can affect the centerword embedding.\"\n",
    "    print(\"The second test passed!\")\n",
    "\n",
    "    # Third test\n",
    "    sampled_negatives = sampled_negatives[0]\n",
    "    sampled_negatives[outside_word_indices.unsqueeze(-1).expand(-1, -1, 5) == 0] = 0\n",
    "    affected_indices = list((set(sampled_negatives.flatten().tolist()) | set(outside_word_indices.flatten().tolist())) - {0})\n",
    "    temp = model.outside_vectors.grad.clone().detach()\n",
    "    temp[affected_indices] = 0.\n",
    "    assert (temp == 0.).all() and (model.outside_vectors.grad[affected_indices] != 0.).all(), \\\n",
    "        \"Only batched outside words and sampled negatives can affect the outside word embedding.\"\n",
    "    print(\"The third test passed!\")\n",
    "\n",
    "    \n",
    "    # forth test\n",
    "    print(loss)\n",
    "    assert loss.detach().allclose(torch.tensor(35.82903290)) or loss.detach().allclose(torch.tensor(24.76907349)), \\\n",
    "        \"Loss of negative sampling do not match expected result.\"\n",
    "    print(\"The forth test passed!\")\n",
    "\n",
    "\n",
    "    print(\"All 4 tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![대체 텍스트](./figures/embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sampling_loss(\n",
    "    center_vectors: torch.Tensor, outside_vectors: torch.Tensor,\n",
    "    center_word_index: torch.Tensor, outside_word_indices: torch.Tensor,\n",
    "    negative_sampler, K: int=10\n",
    ") -> torch.Tensor:\n",
    "    \"\"\" Negative sampling loss function for word2vec models\n",
    "\n",
    "    Implement the negative sampling loss for each pair of (center_word_index, outside_word_indices) in a batch.\n",
    "    As same with naive_softmax_loss, all inputs are batched with batch_size.\n",
    "\n",
    "    Note: Implementing negative sampler is a quite tricky job so we pre-implemented this part. See below comments to check how to use it.\n",
    "    If you want to know how the sampler works, check SkipgramDataset.negative_sampler code in dataset.py file\n",
    "\n",
    "    Arguments/Return Specifications: same as naiveSoftmaxLoss\n",
    "\n",
    "    Additional arguments:\n",
    "    negative_sampler -- the negative sampler\n",
    "    K -- the number of negative samples to take\n",
    "    \"\"\"\n",
    "    assert center_word_index.shape[0] == outside_word_indices.shape[0]\n",
    "\n",
    "    n_tokens, word_dim = center_vectors.shape\n",
    "    batch_size, outside_word_size = outside_word_indices.shape\n",
    "    PAD = SkipgramDataset.PAD_TOKEN_IDX\n",
    "\n",
    "    ##### Sampling negtive indices #####\n",
    "    # Because each outside word needs K negatives samples,\n",
    "    # negative_sampler takes a tensor in shape [batch_size, outside_word_size] and gives a tensor in shape [batch_size, outside_word_size, K]\n",
    "    # where values in last dimension are the indices of sampled negatives for each outside_word.\n",
    "    negative_samples: torch.Tensor = negative_sampler(outside_word_indices, K)\n",
    "    assert negative_samples.shape == torch.Size([batch_size, outside_word_size, K])\n",
    "\n",
    "    ###  YOUR CODE HERE\n",
    "    \n",
    "    # positive sample loss\n",
    "    batch_center_vectors = center_vectors[center_word_index]\n",
    "    batch_dot_product = torch.einsum('bj,kj->bk', [None, None])\n",
    "\n",
    "    batch_true_loss = torch.log(torch.sigmoid(torch.gather(None, 1, None)))\n",
    "    \n",
    "    # negative sample loss\n",
    "    batch_neg_dots = batch_dot_product.gather(1, negative_samples.reshape(None, None * None))\n",
    "    batch_neg_dots = batch_neg_dots.view(None, None, None)    \n",
    "    batch_neg_loss = torch.sum(torch.log(torch.sigmoid(None)), dim=-1)\n",
    "    \n",
    "    loss_matrix = -(None + None)\n",
    "    losses = torch.sum(loss_matrix * (None != 0).int().float(), dim=-1)\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    assert losses.shape == torch.Size([batch_size])\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Negative Sampling Loss Test Case======\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 1, but got NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a0dae183c3c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4321\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtest_neg_sampling_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-fbf6dbf41106>\u001b[0m in \u001b[0;36mtest_neg_sampling_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNegSamplingWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_dimension\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_sampler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnegative_sampler_wrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcenter_word_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutside_word_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-fbf6dbf41106>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, center_word_index, outside_word_indices)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenter_word_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutside_word_indices\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mneg_sampling_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcenter_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutside_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenter_word_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutside_word_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_negative_sampler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_K\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m#############################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-2b87bd6a3882>\u001b[0m in \u001b[0;36mneg_sampling_loss\u001b[1;34m(center_vectors, outside_vectors, center_word_index, outside_word_indices, negative_sampler, K)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# batch_center_vectors, outside_vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mbatch_dot_product\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bj,kj->bk'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# batch_dot_product, outside_word_indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36meinsum\u001b[1;34m(equation, *operands)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;31m# the old interface of passing the operands as one list argument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[0moperands\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moperands\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 1, but got NoneType"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=8)\n",
    "torch.manual_seed(4321)\n",
    "random.seed(4321)\n",
    "\n",
    "test_neg_sampling_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
