{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from dataset import SkipgramDataset\n",
    "\n",
    "#############################################\n",
    "# Helper functions below. DO NOT MODIFY!    #\n",
    "#############################################\n",
    "\n",
    "\n",
    "class Word2Vec(torch.nn.Module, ABC):\n",
    "    \"\"\"\n",
    "    A helper class that wraps your word2vec losses.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_tokens: int, word_dimension: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.center_vectors = torch.nn.Parameter(torch.empty([n_tokens, word_dimension]))\n",
    "        self.outside_vectors = torch.nn.Parameter(torch.empty([n_tokens, word_dimension]))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        torch.nn.init.normal_(self.center_vectors.data)\n",
    "        torch.nn.init.normal_(self.outside_vectors.data)\n",
    "\n",
    "\n",
    "class NaiveWord2Vec(Word2Vec):\n",
    "    def forward(self, center_word_index: torch.Tensor, outside_word_indices: torch.Tensor):\n",
    "        return naive_softmax_loss(self.center_vectors, self.outside_vectors, center_word_index, outside_word_indices)\n",
    "\n",
    "\n",
    "class NegSamplingWord2Vec(Word2Vec):\n",
    "    def __init__(self, n_tokens: int, word_dimension: int, negative_sampler, K: int=10):\n",
    "        super().__init__(n_tokens, word_dimension)\n",
    "\n",
    "        self._negative_sampler = negative_sampler\n",
    "        self._K = K\n",
    "\n",
    "    def forward(self, center_word_index: torch.Tensor, outside_word_indices: torch.Tensor):\n",
    "        return neg_sampling_loss(self.center_vectors, self.outside_vectors, center_word_index, outside_word_indices, self._negative_sampler, self._K)\n",
    "\n",
    "#############################################\n",
    "# Testing functions below.                  #\n",
    "#############################################\n",
    "\n",
    "\n",
    "def test_neg_sampling_loss():\n",
    "    print (\"======Negative Sampling Loss Test Case======\")\n",
    "    center_word_index = torch.randint(1, 100, [5])\n",
    "    outside_word_indices = []\n",
    "    for _ in range(5):\n",
    "        random_window_size = random.randint(3, 6)\n",
    "        outside_word_indices.append([random.randint(1, 99) for _ in range(random_window_size)] + [0] * (6 - random_window_size))\n",
    "    outside_word_indices = torch.Tensor(outside_word_indices).to(torch.long)\n",
    "\n",
    "    neg_sampling_prob = torch.ones([100])\n",
    "    neg_sampling_prob[0] = 0.\n",
    "\n",
    "    dummy_database = type('dummy', (), {'_neg_sample_prob': neg_sampling_prob})\n",
    "\n",
    "    sampled_negatives = list()\n",
    "\n",
    "    def negative_sampler_wrapper(outside_word_indices, K):\n",
    "        result = SkipgramDataset.negative_sampler(dummy_database, outside_word_indices, K)\n",
    "        sampled_negatives.clear()\n",
    "        sampled_negatives.append(result)\n",
    "        return result\n",
    "\n",
    "    model = NegSamplingWord2Vec(n_tokens=100, word_dimension=3, negative_sampler=negative_sampler_wrapper, K=5)\n",
    "\n",
    "    loss = model(center_word_index, outside_word_indices).mean()\n",
    "    loss.backward()\n",
    "\n",
    "    # first test\n",
    "    assert (model.center_vectors.grad[0, :] == 0).all() and (model.outside_vectors.grad[0, :] == 0).all(), \\\n",
    "        \"<PAD> token should not affect the result.\"\n",
    "    print(\"The first test passed! Howerver, this test dosen't guarantee you that <PAD> tokens really don't affects result.\")    \n",
    "\n",
    "    # Second test\n",
    "    temp = model.center_vectors.grad.clone().detach()\n",
    "    temp[center_word_index] = 0.\n",
    "    assert (temp == 0.).all() and (model.center_vectors.grad[center_word_index] != 0.).all(), \\\n",
    "        \"Only batched center words can affect the centerword embedding.\"\n",
    "    print(\"The second test passed!\")\n",
    "\n",
    "    # Third test\n",
    "    sampled_negatives = sampled_negatives[0]\n",
    "    sampled_negatives[outside_word_indices.unsqueeze(-1).expand(-1, -1, 5) == 0] = 0\n",
    "    affected_indices = list((set(sampled_negatives.flatten().tolist()) | set(outside_word_indices.flatten().tolist())) - {0})\n",
    "    temp = model.outside_vectors.grad.clone().detach()\n",
    "    temp[affected_indices] = 0.\n",
    "    assert (temp == 0.).all() and (model.outside_vectors.grad[affected_indices] != 0.).all(), \\\n",
    "        \"Only batched outside words and sampled negatives can affect the outside word embedding.\"\n",
    "    print(\"The third test passed!\")\n",
    "\n",
    "    # forth test\n",
    "    print(loss)\n",
    "    assert loss.detach().allclose(torch.tensor(35.82903290)) or loss.detach().allclose(torch.tensor(24.76907349)), \\\n",
    "        \"Loss of negative sampling do not match expected result.\"\n",
    "    print(\"The forth test passed!\")\n",
    "\n",
    "\n",
    "    print(\"All 4 tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sampling_loss(\n",
    "    center_vectors: torch.Tensor, outside_vectors: torch.Tensor,\n",
    "    center_word_index: torch.Tensor, outside_word_indices: torch.Tensor,\n",
    "    negative_sampler, K: int=10\n",
    ") -> torch.Tensor:\n",
    "    \"\"\" Negative sampling loss function for word2vec models\n",
    "\n",
    "    Implement the negative sampling loss for each pair of (center_word_index, outside_word_indices) in a batch.\n",
    "    As same with naive_softmax_loss, all inputs are batched with batch_size.\n",
    "\n",
    "    Note: Implementing negative sampler is a quite tricky job so we pre-implemented this part. See below comments to check how to use it.\n",
    "    If you want to know how the sampler works, check SkipgramDataset.negative_sampler code in dataset.py file\n",
    "\n",
    "    Arguments/Return Specifications: same as naiveSoftmaxLoss\n",
    "\n",
    "    Additional arguments:\n",
    "    negative_sampler -- the negative sampler\n",
    "    K -- the number of negative samples to take\n",
    "    \"\"\"\n",
    "    assert center_word_index.shape[0] == outside_word_indices.shape[0]\n",
    "\n",
    "    n_tokens, word_dim = center_vectors.shape\n",
    "    batch_size, outside_word_size = outside_word_indices.shape\n",
    "    PAD = SkipgramDataset.PAD_TOKEN_IDX\n",
    "\n",
    "    ##### Sampling negtive indices #####\n",
    "    # Because each outside word needs K negatives samples,\n",
    "    # negative_sampler takes a tensor in shape [batch_size, outside_word_size] and gives a tensor in shape [batch_size, outside_word_size, K]\n",
    "    # where values in last dimension are the indices of sampled negatives for each outside_word.\n",
    "    negative_samples: torch.Tensor = negative_sampler(outside_word_indices, K)\n",
    "    assert negative_samples.shape == torch.Size([batch_size, outside_word_size, K])\n",
    "\n",
    "    ###  YOUR CODE HERE\n",
    "    batch_center_vectors = center_vectors[center_word_index]\n",
    "    batch_dot_product = torch.einsum('bj,kj->bk', [batch_center_vectors, outside_vectors])\n",
    "\n",
    "    batch_true_loss = torch.log(torch.sigmoid(torch.gather(batch_dot_product, 1, outside_word_indices)))\n",
    "\n",
    "    batch_neg_dots = batch_dot_product.gather(1, negative_samples.reshape(batch_size, outside_word_size * K))\n",
    "    batch_neg_dots = batch_neg_dots.view(batch_size, outside_word_size, K)\n",
    "    batch_neg_loss = torch.sum(torch.log(torch.sigmoid(-batch_neg_dots)), dim=-1)\n",
    "\n",
    "    loss_matrix = -(batch_true_loss + batch_neg_loss)\n",
    "    losses = torch.sum(loss_matrix * (outside_word_indices != 0).int().float(), dim=-1)\n",
    "    ### END YOUR CODE\n",
    "    assert losses.shape == torch.Size([batch_size])\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Negative Sampling Loss Test Case======\n",
      "The first test passed! Howerver, this test dosen't guarantee you that <PAD> tokens really don't affects result.\n",
      "The second test passed!\n",
      "The third test passed!\n",
      "tensor(24.76907349, grad_fn=<MeanBackward0>)\n",
      "The forth test passed!\n",
      "All 4 tests passed!\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=8)\n",
    "torch.manual_seed(4321)\n",
    "random.seed(4321)\n",
    "\n",
    "test_neg_sampling_loss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "0810_word_embedding",
   "language": "python",
   "name": "0810_word_embedding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
